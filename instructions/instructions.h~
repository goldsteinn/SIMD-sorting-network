#ifndef _INSTRUCTIONS_H_
#define _INSTRUCTIONS_H_


namespace vop {
enum INSTRUCTION_SET { AVX = 0, AVX2 = 1, AVX512 = 2 };
enum OPTIMIZATION { ON = 0, OFF = 1 };

namespace internal {
struct avail_instructions {
#if defined __AVX512F__
    static constexpr uint32_t AVX512F = 1;
#else
    static constexpr uint32_t AVX512F  = 0;
#endif

#if defined __AVX512VL__
    static constexpr uint32_t AVX512VL = 1;
#else
    static constexpr uint32_t AVX512VL = 0;
#endif

#if defined __AVX2__
    static constexpr uint32_t AVX2 = 1;
#else
    static constexpr uint32_t AVX2     = 0;
#endif

#if defined __AVX__
    static constexpr uint32_t AVX = 1;
#else
    static constexpr uint32_t AVX      = 0;
#endif

#if defined __SSE2__
    static constexpr uint32_t SSE2 = 1;
#else
    static constexpr uint32_t SSE2     = 0;
#endif

#if defined __SSE3__
    static constexpr uint32_t SSE3 = 1;
#else
    static constexpr uint32_t SSE3     = 0;
#endif

#if defined __SSE4_1__
    static constexpr uint32_t SSE4_1 = 1;
#else
    static constexpr uint32_t SSE4_1   = 0;
#endif

#if defined __SSE4_2__
    static constexpr uint32_t SSE4_2 = 1;
#else
    static constexpr uint32_t SSE4_2   = 0;
#endif
};

template<typename T, INSTRUCTION_SET ops, uint32_t vec_size>
struct vector_ops_support_impl {

    template<uint32_t... e>
    static constexpr uint64_t
    build_blend_mask() {
        constexpr uint32_t perm[vec_size] = { static_cast<uint32_t>(e)... };

        uint64_t blend_mask = 0;

        for (uint32_t i = 0; i < n; ++i) {
            if (perm[i] < (n - (i + 1))) {
                if constexpr (sizeof(T) < sizeof(uint64_t)) {
                    blend_mask |= ((1UL) << i);
                }
                else /* sizeof(T) == sizeof(uint64_t) */ {
                    blend_mask |= ((3UL) << (2 * i));
                }
            }
        }
    }

    template<uint64_t blend_mask, uint32_t... seq>
    static constexpr decltype(auto)
    build_blend_vec_initializer_kernel(
        std::integer_sequence<uint32_t, seq...> _seq) {
        return std::integer_sequence<uint32_t,
                                     !!(blend_mask & (1UL) << seq)...>{};
    }

    // this is for epi8 blend
    template<uint64_t blend_mask>
    static constexpr decltype(auto)
    build_blend_vec_initializer() {
        return build_blend_vec_kernel<blend_mask>(
            std::make_integer_sequence<uint32_t, vec_size>{});
    }

    template<uint32_t offset, uint32_t ele_per_lane, uint32_t... e>
    static constexpr uint64_t
    build_shuffle_mask_impl() {
        constexpr uint32_t perms[n] = { static_cast<uint32_t>(e)... };
        uint64_t           mask     = 0;
        for (uint32_t i = 0; i < vec_size; i += ele_per_lane) {
            uint64_t lane_mask   = 0;
            uint32_t lower_bound = vec_size - (i + ele_per_lane + offset);
            uint32_t upper_bound = vec_size - (i + ele_per_lane + offset);
            for (uint32_t j = 0; j < ele_per_lane; ++j) {
                uint32_t p = perms[i + j + offset];
                if (p >= lower_bound && p < upper_bound) {
                    uint64_t idx = p - lower_bound;
                    uint32_t slot =
                        (ele_per_lane - (j + 1)) * ulog2(ele_per_lane);
                    lane_mask |= (idx << slot);
                }
                else {
                    // moving elements outside of lane so cant use shuffle
                    return 0;
                }
            }
            mask |= (lane_mask << i);
        }
    }

    static constexpr uint64_t
    build_shuffle_mask() {

    }
};

}  // namespace internal

template<typename T, INSTRUCTION_SET ops, OPTIMIZATION opt, uint32_t vec_size>
struct vector_ops;

template<typename T, INSTRUCTION_SET ops, OPTIMIZATION opt>
struct vector_ops<T, ops, opt, sizeof(__m128i)> {
    using vec_t = __m128i;


    vec_t ALWAYS_INLINE CONST_ATTR
    vec_min(vec_t v1, vec_t v2) {
        if constexpr (sizeof(T) == sizeof(uint8_t)) {
            if constexpr (std::is_signed<T>::value) {
                // SSE4.1
                return _mm_min_epi8(v1, v2);
            }
            else {
                // SSE4.1
                return _mm_min_epu8(v1, v2);
            }
        }
        else if constexpr (sizeof(T) == sizeof(uint16_t)) {
            if constexpr (std::is_signed<T>::value) {
                // SSE2
                return _mm_min_epi16(v1, v2);
            }
            else {
                // SSE2
                return _mm_min_epu16(v1, v2);
            }
        }
        else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            if constexpr (std::is_signed<T>::value) {
                // SSE4.1
                return _mm_min_epi32(v1, v2);
            }
            else {
                // SSE4.1
                return _mm_min_epu32(v1, v2);
            }
        }
        else /* sizeof(T) == sizeof(uint64_t) */ {

            if constexpr (ops >= INSTRUCTION_SET::AVX512 &&
                          avail_instructions::AVX512F &&
                          avail_instructions::AVX512VL) {
                if constexpr (std::is_signed<T>::value) {
                    // AVX512F & AVX512VL
                    return _mm_min_epi64(v1, v2);
                }
                else {
                    // AVX512F & AVX512VL
                    return _mm_min_epu64(v1, v2);
                }
            }
            else {
                if constexpr (std::is_signed<T>::value) {
                    // SSE4.2
                    vec_t cmp_mask = _mm_cmpgt_epi64(v1, v2);
                    // SSE4.1
                    return _mm_blendv_epi8(v2, v1, cmp_mask);
                }
                else {
                    // SSE2
                    vec_t sign_bit = sign_bit = _mm_set1_epi64x((1UL) << 63);
                    // SSE4.2 & SSE2
                    vec_t cmp_mask =
                        _mm_cmpgt_epi64(_mm_xor_si128(v1, sign_bits),
                                        _mm_xor_si128(v2, sign_bits))
                        // SSE4.1
                        return _mm_blendv_epi8(v2, v1, cmp_mask);
                }
            }
        }
    }


    vec_t ALWAYS_INLINE CONST_ATTR
    vec_max(vec_t v1, vec_t v2) {
        if constexpr (sizeof(T) == sizeof(uint8_t)) {
            if constexpr (std::is_signed<T>::value) {
                // SSE4.1
                return _mm_max_epi8(v1, v2);
            }
            else {
                // SSE4.1
                return _mm_max_epu8(v1, v2);
            }
        }
        else if constexpr (sizeof(T) == sizeof(uint16_t)) {
            if constexpr (std::is_signed<T>::value) {
                // SSE2
                return _mm_max_epi16(v1, v2);
            }
            else {
                // SSE2
                return _mm_max_epu16(v1, v2);
            }
        }
        else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            if constexpr (std::is_signed<T>::value) {
                // SSE4.1
                return _mm_max_epi32(v1, v2);
            }
            else {
                // SSE4.1
                return _mm_max_epu32(v1, v2);
            }
        }
        else /* sizeof(T) == sizeof(uint64_t) */ {

            if constexpr (ops >= INSTRUCTION_SET::AVX512 &&
                          avail_instructions::AVX512F &&
                          avail_instructions::AVX512VL) {
                if constexpr (std::is_signed<T>::value) {
                    // AVX512F & AVX512VL
                    return _mm_max_epi64(v1, v2);
                }
                else {
                    // AVX512F & AVX512VL
                    return _mm_max_epu64(v1, v2);
                }
            }
            else {
                if constexpr (std::is_signed<T>::value) {
                    // SSE4.2
                    vec_t cmp_mask = _mm_cmpgt_epi64(v1, v2);
                    // SSE4.1
                    return _mm_blendv_epi8(v1, v2, cmp_mask);
                }
                else {
                    // SSE2
                    vec_t sign_bit = sign_bit = _mm_set1_epi64x((1UL) << 63);
                    // SSE4.2 & SSE2
                    vec_t cmp_mask =
                        _mm_cmpgt_epi64(_mm_xor_si128(v1, sign_bits),
                                        _mm_xor_si128(v2, sign_bits))
                        // SSE4.1
                        return _mm_blendv_epi8(v1, v2, cmp_mask);
                }
            }
        }
    }

    template<uint64_t blend_mask, T... e>
    vec_t ALWAYS_INLINE CONST_ATTR
    vec_blend(vec_t v1, vec_t v2) {

        if constexpr (sizeof(T) == sizeof(uint8_t)) {
            if constexpr (avail_instructions::AVX512VL &&
                          avail_instructions::AVX512BW) {
                // AVX512VL & AVX512BW
                return _mm_mask_mov_epi8(v1, blend_mask, v2);
            }
            else {
                // TODO
            }
        }
        else if constexpr (sizeof(T) == sizeof(uint16_t)) {
            if constexpr (avail_instructions::AVX512F &&
                          avail_instructions::AVX512VL) {
                // AVX512F & AVX512VL
                return _mm_mask_mov_epi16(v1, blend_mask, v2);
            }
            else {
                // SSE4.1
                return _mm_blend_epi16(v1, v2, blend_mask);
            }
        }
        else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            // AVX2
            return _mm_blend_epi32(v1, v2, blend_mask);
        }
        else /* sizeof(T) == sizeof(uint64_t) */ {
            if constexpr (avail_instructions::AVX512F &&
                          avail_instructions::AVX512VL) {
                // AVX512F & AVX512VL
                return _mm_mask_mov_epi64(v1, blend_mask, v2);
            }
            else {
                // build_blend_mask will create proper mask for epi64 if AVX512F
                // and AVX512VL are not available

                // AVX2
                return _mm_blend_epi32(v1, blend_mask, v2);
            }
        }
    }

    template<uint64_t shuffle_mask, T... e>
    vec_t ALWAYS_INLINE CONST_ATTR
    vec_permutate(vec_t v) {
        if constexpr (sizeof(T) == sizeof(uint8_t)) {
            // SSE3
            return _mm_shuffle_epi8(v, _mm_set_epi8(e...));
        }
        else if constexpr (sizeof(T) == sizeof(uint16_t)) {
            constexpr uint32_t shuffle_mask_lo = shuffle_mask;
            constexpr uint32_t shuffle_mask_hi = (shuffle_mask >> 32);

            // SSE 2
            return _mm_shufflehi_epi16(_mm_shufflelo_epi16(v, shuffle_mask_lo),
                                       shuffle_mask_hi);
        }
        else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            // SSE2
            return _mm_shuffle_epi32(v, shuffle_mask);
        }
        else /* sizeof(T) == sizeof(uint64_t) */ {
            // SSE2
            return _mm_shuffle_epi32(v, shuffle_mask);
        }
    }
};

template<INSTRUCTION_SET ops>
struct vector_ops<ops, sizeof(__m256i)>;

template<INSTRUCTION_SET ops>
struct vector_ops<ops, sizeof(__m512i)>;


template<typename T, uint32_t n, INSTRUCTION_SET ops>
struct vector_ops {};


}  // namespace vop


#endif
