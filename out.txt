[1][8]
DO_FULL_LOAD: True
__m64 [TMP0] = _mm_set1_pi8(uint8_t(0xff));
memcpy(&[TMP0], [ARR], 8);
[TMP0]
_mm_shuffle_epi8([V], _mm_set_pi8(6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_pu8([V1], [V2])
_mm_min_pu8([V1], [V2])
__m64 [TMP0] = (__m64)0xff00ff00ff00ff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shuffle_epi8([V], _mm_set_pi8(4, 5, 6, 7, 0, 1, 2, 3))
_mm_min_pu8([V1], [V2])
_mm_min_pu8([V1], [V2])
__m64 [TMP0] = (__m64)0xffff0000ffff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shuffle_epi8([V], _mm_set_pi8(6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_pu8([V1], [V2])
_mm_min_pu8([V1], [V2])
__m64 [TMP0] = (__m64)0xff00ff00ff00ff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shuffle_epi8([V], _mm_set_pi8(0, 1, 2, 3, 4, 5, 6, 7))
_mm_min_pu8([V1], [V2])
_mm_min_pu8([V1], [V2])
__m64 [TMP0] = (__m64)0xffffffff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shuffle_epi8([V], _mm_set_pi8(5, 4, 7, 6, 1, 0, 3, 2))
_mm_min_pu8([V1], [V2])
_mm_min_pu8([V1], [V2])
__m64 [TMP0] = (__m64)0xffff0000ffff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shuffle_epi8([V], _mm_set_pi8(6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_pu8([V1], [V2])
_mm_min_pu8([V1], [V2])
__m64 [TMP0] = (__m64)0xff00ff00ff00ff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
[1][9]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 7, 8, 6, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 6, 7, 8, 5, 4, 0, 1, 2, 3))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 5, 6, 7, 8, 4, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 3, 4, 5, 6, 7, 8, 2, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 6, 7, 4, 5, 3, 2, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x1)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xe4)
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x3)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][10]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 8, 9, 7, 5, 6, 3, 4, 2, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 7, 8, 9, 6, 5, 4, 2, 3, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 6, 7, 8, 9, 5, 3, 4, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 5, 6, 7, 8, 2, 1, 4, 3, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 3, 4, 1, 2, 5, 8, 9, 6, 7, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 8, 9, 7, 6, 3, 0, 5, 2, 1, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xe4)
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x3)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][11]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 9, 10, 8, 6, 7, 5, 3, 4, 2, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 8, 9, 10, 7, 5, 6, 4, 2, 3, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 8, 9, 6, 7, 5, 3, 4, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 6, 7, 8, 9, 10, 5, 2, 1, 4, 3, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 9, 10, 6, 5, 8, 7, 3, 4, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 2, 3, 7, 8, 5, 6, 4, 9, 10, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 4, 1, 2, 3, 8, 5, 6, 7, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 8, 9, 10, 5, 6, 7, 0, 1, 2, 3, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 8, 9, 7, 4, 5, 6, 3, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][12]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 10, 11, 9, 7, 8, 6, 4, 5, 3, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 9, 10, 11, 8, 6, 7, 5, 3, 4, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 9, 10, 7, 8, 6, 4, 5, 3, 2, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 7, 8, 9, 10, 11, 6, 5, 0, 1, 2, 3, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 10, 11, 7, 6, 9, 8, 3, 2, 5, 4, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 8, 9, 6, 7, 4, 5, 2, 3, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 2, 3, 4, 5, 7, 6, 8, 9, 10, 11, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x3)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 9, 8, 11, 10, 2, 3, 1, 0, 6, 7, 5, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x5)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 10, 11, 8, 9, 4, 5, 6, 7, 1, 0, 3, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][13]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 11, 12, 9, 10, 7, 8, 6, 4, 5, 3, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 9, 10, 11, 12, 8, 6, 7, 5, 3, 4, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 11, 12, 9, 10, 7, 8, 6, 4, 5, 3, 2, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 6, 7, 8, 9, 10, 11, 12, 5, 0, 1, 2, 3, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 10, 11, 12, 7, 6, 9, 8, 3, 2, 5, 4, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 1, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 12, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 2, 3, 4, 5, 7, 6, 8, 9, 10, 11, 1, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x3)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 8, 9, 10, 11, 12, 2, 3, 1, 0, 6, 7, 5, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x5)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 8, 9, 10, 4, 5, 6, 7, 1, 0, 3, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][14]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 12, 13, 10, 11, 8, 9, 7, 5, 6, 3, 4, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 10, 11, 12, 13, 9, 7, 8, 3, 4, 5, 6, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 12, 13, 10, 11, 8, 9, 7, 5, 6, 3, 4, 2, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 7, 8, 9, 10, 11, 12, 13, 6, 0, 1, 2, 3, 4, 5))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 11, 12, 13, 8, 7, 10, 9, 4, 3, 6, 5, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0x4e), 0xc6)
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x5)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 12, 13, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][15]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi8(uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 11, 12, 13, 14, 7, 8, 9, 10, 3, 4, 5, 6, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 2, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 7, 8, 9, 10, 11, 12, 13, 14, 6, 0, 1, 2, 3, 4, 5))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 12, 11, 14, 13, 8, 7, 10, 9, 4, 3, 6, 5, 0, 1, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 10, 9, 8, 11, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 12, 13, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
[1][16]
DO_FULL_LOAD: True
_mm_loadu_si128((__m128i *)[ARR])
_mm_shuffle_epi8([V], _mm_set_epi8(14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0xf)
_mm_shuffle_epi8([V], _mm_set_epi8(14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x5)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xb1)
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0xf)
_mm_shuffle_epi8([V], _mm_set_epi8(14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
_mm_shuffle_epi8([V], _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x3)
_mm_shuffle_epi32([V], 0xb1)
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x5)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xb1)
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blend_epi16([V1], [V2], 0xf)
_mm_shuffle_epi8([V], _mm_set_epi8(14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm_min_epu8([V1], [V2])
_mm_min_epu8([V1], [V2])
_mm_blendv_epi8([V1], [V2], _mm_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
[1][17]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 128, 14, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 15, 128, 13, 12, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 14, 15, 128, 12, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 12, 13, 14, 15, 128, 10, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 11, 128, 128, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 11, 10, 9, 8, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 8, 9, 10, 11, 12, 13, 14, 15, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 13, 12, 15, 14, 9, 8, 11, 10, 128, 6, 5, 4, 3, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 7, 128, 128, 128, 128, 128, 128, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 7, 6, 5, 4, 3, 2, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][18]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 16, 17, 15, 13, 14, 11, 12, 9, 10, 7, 8, 6, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 16, 128, 14, 13, 9, 10, 11, 12, 8, 6, 7, 5, 4, 0, 1, 2, 3));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 128, 128, 128, 13, 11, 12, 9, 10, 7, 8, 4, 5, 6, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 128, 14, 15, 128, 128, 11, 10, 9, 6, 5, 8, 7, 0, 3, 2, 1, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 12, 13, 128, 128, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 128, 13, 14, 12, 11, 10, 9, 7, 8, 5, 6, 4, 3, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 10, 11, 12, 13, 14, 15, 128, 4, 1, 2, 3, 8, 5, 6, 7, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 9, 128, 128, 128, 128, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 13, 128, 15, 10, 9, 12, 11, 6, 5, 8, 7, 2, 1, 4, 3, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 128, 1, 2, 3, 4, 5, 6, 9, 128, 128, 10, 11, 12, 13, 14, 15, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 7, 8, 128, 128, 128, 128, 128, 128, 128, 16, 17, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 16, 17, 11, 10, 13, 12, 15, 14, 7, 0, 9, 2, 1, 4, 3, 6, 5, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 9, 8, 11, 10, 13, 12, 3, 6, 5, 0, 7, 2, 1, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][19]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 18, 16, 14, 15, 12, 13, 11, 9, 10, 7, 8, 6, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 16, 17, 18, 15, 14, 13, 11, 12, 10, 9, 8, 6, 7, 5, 4, 0, 1, 2, 3))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 128, 16, 17, 128, 14, 12, 13, 9, 10, 11, 7, 8, 4, 5, 6, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 128, 128, 128, 11, 10, 13, 12, 9, 6, 5, 8, 7, 0, 3, 2, 1, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 16, 17, 14, 15, 12, 13, 10, 11, 9, 7, 8, 5, 6, 4, 3, 2, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 128, 128, 128, 11, 14, 128, 128, 15, 128, 9, 4, 1, 2, 3, 8, 5, 6, 7, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 12, 13, 10, 128, 128, 17, 18, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 18, 16, 15, 12, 9, 14, 11, 10, 13, 6, 5, 8, 7, 2, 1, 4, 3, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 13, 128, 15, 10, 9, 12, 11, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 128, 128, 128, 128, 13, 14, 11, 12, 9, 10, 8, 128, 128, 5, 4, 3, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 6, 7, 15, 16, 128, 128, 128, 128, 128, 128, 128, 17, 18, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 128, 1, 2, 3, 4, 5, 6, 7, 128, 9, 10, 11, 12, 13, 14, 15, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 8, 128, 128, 128, 128, 128, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 16, 17, 18, 11, 10, 9, 12, 15, 14, 13, 0, 3, 2, 1, 4, 7, 6, 5, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 16, 17, 13, 14, 15, 8, 9, 10, 11, 12, 5, 6, 7, 0, 1, 2, 3, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 12, 13, 14, 11, 8, 9, 10, 7, 4, 5, 6, 3, 0, 1, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][20]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 17, 128, 128, 13, 14, 12, 10, 11, 8, 9, 7, 5, 6, 3, 4, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 18, 19, 16, 15, 14, 12, 13, 11, 10, 9, 7, 8, 6, 5, 2, 3, 4, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 16, 17, 18, 19, 15, 13, 14, 10, 11, 12, 8, 9, 5, 6, 7, 1, 2, 3, 4, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 128, 16, 17, 128, 12, 11, 14, 13, 10, 7, 6, 9, 8, 5, 4, 0, 1, 2, 3));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 18, 128, 128, 13, 14, 11, 12, 10, 8, 9, 6, 7, 5, 4, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 128, 128, 128, 128, 15, 128, 128, 128, 128, 10, 9, 2, 3, 0, 1, 4, 7, 8, 5, 6));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 14, 11, 12, 128, 18, 19, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 17, 16, 13, 10, 15, 12, 11, 14, 5, 8, 7, 4, 9, 6, 3, 2, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 128, 128, 128, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 128, 128, 128, 128, 2, 3, 4, 5, 11, 10, 128, 128, 128, 128, 12, 13, 14, 15, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 6, 7, 8, 9, 128, 128, 128, 128, 128, 128, 16, 17, 18, 19, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 16, 19, 18, 15, 14, 13, 12, 6, 7, 1, 0, 10, 11, 5, 4, 3, 2, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 16, 17, 11, 10, 8, 9, 15, 14, 12, 13, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 8, 9, 10, 11, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][21]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 19, 20, 18, 16, 17, 15, 13, 14, 12, 10, 11, 8, 9, 7, 5, 6, 3, 4, 2, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 18, 19, 20, 17, 128, 128, 14, 12, 13, 11, 10, 9, 7, 8, 6, 5, 2, 3, 4, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 16, 17, 15, 13, 14, 10, 11, 12, 8, 9, 5, 6, 7, 1, 2, 3, 4, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 16, 17, 18, 19, 20, 15, 12, 11, 14, 13, 10, 7, 6, 9, 8, 5, 4, 0, 1, 2, 3))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 19, 20, 16, 128, 18, 128, 13, 14, 11, 12, 10, 8, 9, 6, 7, 5, 4, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 128, 128, 17, 18, 128, 128, 14, 128, 128, 11, 10, 9, 2, 3, 0, 1, 4, 7, 8, 5, 6));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 12, 13, 128, 128, 15, 16, 128, 19, 20, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 128, 128, 128, 13, 128, 15, 128, 128, 10, 5, 8, 7, 4, 9, 6, 3, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 11, 12, 128, 18, 128, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 18, 19, 20, 128, 16, 128, 10, 11, 12, 13, 14, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 17, 128, 15, 128, 13, 10, 11, 12, 8, 9, 6, 7, 4, 5, 2, 3, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 128, 128, 128, 16, 17, 14, 15, 12, 13, 10, 11, 9, 8, 128, 128, 128, 4, 3, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 5, 6, 7, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 18, 19, 20, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 128, 128, 2, 3, 4, 5, 6, 7, 128, 128, 10, 11, 12, 13, 14, 15, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 8, 9, 128, 128, 128, 128, 128, 128, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 16, 17, 18, 19, 20, 11, 10, 13, 12, 15, 14, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 16, 17, 18, 15, 14, 8, 9, 11, 10, 12, 13, 7, 6, 1, 0, 3, 2, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 16, 17, 13, 12, 15, 14, 8, 9, 10, 11, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][22]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 20, 21, 19, 17, 18, 16, 14, 15, 13, 11, 12, 9, 10, 8, 6, 7, 5, 3, 4, 2, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 19, 20, 21, 18, 16, 17, 15, 13, 14, 12, 11, 10, 8, 9, 5, 6, 7, 2, 3, 4, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 19, 20, 17, 18, 16, 14, 15, 11, 12, 13, 9, 10, 8, 7, 5, 6, 1, 2, 3, 4, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 17, 18, 19, 20, 21, 16, 13, 12, 15, 14, 11, 10, 5, 6, 7, 8, 9, 4, 0, 1, 2, 3))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 20, 21, 17, 16, 19, 18, 14, 15, 12, 13, 11, 8, 7, 10, 9, 5, 6, 4, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 128, 128, 18, 19, 16, 17, 15, 128, 128, 12, 11, 9, 10, 7, 8, 1, 2, 4, 3, 5, 6, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 14, 128, 128, 128, 128, 128, 20, 21, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 128, 128, 128, 128, 128, 128, 128, 128, 11, 6, 5, 3, 0, 10, 9, 4, 8, 2, 1, 7));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 12, 13, 14, 19, 16, 17, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 19, 20, 21, 16, 17, 18, 11, 12, 13, 14, 15, 8, 9, 10, 4, 3, 5, 7, 6, 0, 1, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 19, 20, 18, 128, 16, 128, 14, 11, 12, 13, 10, 7, 8, 9, 6, 4, 5, 3, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 128, 128, 128, 128, 128, 128, 3, 4, 13, 12, 11, 128, 128, 128, 128, 128, 128, 14, 15, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 5, 6, 7, 8, 9, 10, 128, 128, 128, 128, 128, 16, 17, 18, 19, 20, 21, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 17, 16, 19, 18, 21, 20, 15, 14, 5, 6, 7, 2, 1, 0, 11, 12, 13, 4, 3, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 20, 21, 17, 16, 19, 18, 11, 8, 9, 10, 15, 12, 13, 14, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 16, 17, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][23]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 21, 22, 20, 18, 19, 17, 128, 128, 14, 12, 13, 11, 9, 10, 8, 6, 7, 5, 3, 4, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 20, 21, 22, 19, 17, 18, 16, 14, 15, 11, 12, 13, 10, 8, 9, 5, 6, 7, 2, 3, 4, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 20, 21, 18, 19, 17, 128, 128, 14, 13, 11, 12, 9, 10, 8, 7, 5, 6, 1, 2, 3, 4, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 18, 19, 20, 21, 22, 17, 16, 11, 12, 13, 14, 15, 10, 5, 6, 7, 8, 9, 4, 0, 1, 2, 3))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 21, 22, 18, 17, 20, 19, 128, 13, 128, 15, 11, 12, 8, 7, 10, 9, 5, 6, 4, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 19, 20, 17, 18, 128, 128, 13, 14, 12, 11, 9, 10, 7, 8, 1, 2, 4, 3, 5, 6, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 128, 128, 128, 16, 18, 17, 19, 128, 128, 128, 12, 11, 6, 5, 3, 0, 10, 9, 4, 8, 2, 1, 7));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 14, 15, 128, 128, 128, 128, 20, 21, 22, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 20, 19, 22, 21, 128, 128, 128, 11, 128, 128, 128, 15, 8, 9, 10, 4, 3, 5, 7, 6, 0, 1, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 14, 12, 128, 17, 18, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 21, 22, 19, 20, 128, 16, 17, 128, 12, 11, 14, 13, 10, 7, 8, 9, 6, 4, 5, 3, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 128, 128, 128, 128, 128, 128, 128, 3, 14, 13, 12, 11, 128, 128, 128, 128, 128, 128, 128, 15, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 4, 5, 6, 7, 8, 9, 10, 128, 128, 128, 128, 128, 16, 17, 18, 19, 20, 21, 22, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 18, 17, 16, 19, 22, 21, 20, 15, 4, 5, 6, 7, 2, 1, 0, 11, 12, 13, 14, 3, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 20, 21, 22, 17, 16, 19, 18, 11, 8, 9, 10, 15, 12, 13, 14, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 20, 21, 18, 19, 16, 17, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][24]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 21, 19, 20, 18, 16, 17, 15, 13, 14, 12, 10, 11, 9, 7, 8, 6, 4, 5, 3, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 22, 23, 20, 18, 19, 17, 128, 128, 12, 13, 14, 11, 9, 10, 6, 7, 8, 3, 4, 5, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 21, 22, 19, 20, 18, 16, 17, 15, 14, 12, 13, 10, 11, 9, 8, 6, 7, 5, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 20, 21, 22, 23, 18, 17, 128, 13, 14, 15, 128, 11, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 12, 128, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 19, 18, 21, 20, 128, 128, 128, 128, 12, 13, 9, 8, 11, 10, 6, 7, 4, 5, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 20, 21, 18, 19, 16, 17, 14, 15, 13, 12, 10, 11, 8, 9, 7, 6, 5, 4, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 128, 128, 16, 17, 19, 18, 20, 21, 128, 128, 13, 12, 11, 10, 0, 1, 2, 3, 5, 4, 6, 7, 8, 9));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 128, 128, 128, 128, 128, 128, 22, 23, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 20, 23, 22, 128, 128, 128, 128, 128, 128, 128, 128, 7, 6, 4, 5, 11, 10, 8, 9, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 13, 12, 18, 19, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 20, 21, 16, 17, 18, 19, 13, 12, 15, 14, 9, 8, 11, 10, 4, 5, 6, 7, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 3, 2, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 13, 12, 128, 128, 128, 128, 128, 128, 128, 128, 3, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 4, 5, 6, 7, 8, 9, 10, 11, 128, 128, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x6)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 18, 17, 16, 23, 22, 21, 20, 4, 5, 6, 7, 3, 2, 1, 0, 12, 13, 14, 15, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 20, 23, 22, 17, 16, 19, 18, 8, 9, 10, 11, 12, 13, 14, 15, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 20, 21, 18, 19, 16, 17, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][25]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 23, 24, 21, 22, 19, 20, 18, 16, 17, 15, 13, 14, 12, 10, 11, 9, 7, 8, 6, 4, 5, 3, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 21, 22, 23, 24, 20, 18, 19, 17, 128, 128, 12, 13, 14, 11, 9, 10, 6, 7, 8, 3, 4, 5, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 23, 24, 21, 22, 19, 20, 18, 16, 17, 15, 14, 12, 13, 10, 11, 9, 8, 6, 7, 5, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 18, 19, 20, 21, 22, 23, 24, 17, 128, 13, 14, 15, 128, 11, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 12, 128, 128, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 22, 23, 24, 19, 18, 21, 20, 128, 128, 128, 128, 12, 13, 9, 8, 11, 10, 6, 7, 4, 5, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 128, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 128, 12, 10, 11, 8, 9, 7, 6, 5, 4, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 13, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 24, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 128, 128, 16, 17, 19, 18, 20, 21, 128, 128, 13, 12, 11, 10, 0, 1, 2, 3, 5, 4, 6, 7, 8, 9));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 128, 128, 128, 128, 128, 128, 22, 23, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 20, 21, 22, 23, 24, 128, 128, 128, 128, 128, 128, 128, 128, 7, 6, 4, 5, 11, 10, 8, 9, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 13, 12, 18, 19, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 20, 21, 22, 16, 17, 18, 19, 13, 12, 15, 14, 9, 8, 11, 10, 4, 5, 6, 7, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 128, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 128, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 3, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 24, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 13, 12, 128, 128, 128, 128, 128, 128, 128, 128, 3, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 4, 5, 6, 7, 8, 9, 10, 11, 128, 128, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x6)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 16, 19, 18, 17, 20, 23, 22, 21, 24, 4, 5, 6, 7, 3, 2, 1, 0, 12, 13, 14, 15, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 22, 23, 16, 17, 18, 19, 20, 8, 9, 10, 11, 12, 13, 14, 15, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 20, 21, 22, 19, 16, 17, 18, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][26]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 24, 25, 22, 23, 20, 21, 19, 17, 18, 16, 14, 15, 13, 11, 12, 9, 10, 7, 8, 6, 4, 5, 3, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 22, 23, 24, 25, 21, 19, 20, 18, 16, 17, 13, 14, 15, 9, 10, 11, 12, 6, 7, 8, 3, 4, 5, 2, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 24, 25, 22, 23, 20, 21, 19, 17, 18, 16, 15, 13, 14, 11, 12, 9, 10, 8, 6, 7, 5, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 19, 20, 21, 22, 23, 24, 25, 18, 128, 128, 15, 128, 128, 12, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 14, 128, 16, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 23, 24, 25, 20, 19, 22, 21, 16, 128, 18, 128, 13, 14, 10, 9, 12, 11, 6, 7, 8, 4, 5, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 128, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 128, 13, 11, 12, 9, 10, 7, 8, 4, 5, 6, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 14, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 25, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 128, 16, 17, 18, 20, 19, 21, 22, 23, 128, 14, 13, 12, 11, 0, 1, 2, 3, 6, 5, 4, 7, 8, 9, 10));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 128, 128, 128, 128, 128, 24, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 21, 22, 23, 24, 25, 128, 16, 128, 128, 19, 128, 128, 128, 8, 7, 6, 5, 12, 11, 10, 9, 0, 1, 2, 3, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 14, 13, 128, 20, 18, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 22, 23, 17, 18, 19, 20, 128, 13, 128, 15, 10, 9, 12, 11, 6, 5, 8, 7, 2, 3, 4, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 13, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 2, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 128, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 5, 6, 7, 4, 3, 2, 1, 0, 13, 14, 15, 12, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 24, 25, 23, 22, 17, 16, 19, 18, 21, 20, 9, 10, 11, 8, 13, 14, 15, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xffffffff)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][27]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 25, 26, 23, 24, 21, 22, 20, 18, 19, 16, 17, 14, 15, 13, 11, 12, 9, 10, 7, 8, 6, 4, 5, 3, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 23, 24, 25, 26, 22, 20, 21, 16, 17, 18, 19, 13, 14, 15, 9, 10, 11, 12, 6, 7, 8, 3, 4, 5, 2, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 25, 26, 23, 24, 21, 22, 20, 18, 19, 16, 17, 15, 13, 14, 11, 12, 9, 10, 8, 6, 7, 5, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 20, 21, 22, 23, 24, 25, 26, 19, 128, 128, 128, 128, 128, 128, 12, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 13, 14, 15, 16, 17, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 24, 25, 26, 21, 20, 23, 22, 17, 16, 19, 18, 13, 14, 15, 10, 9, 12, 11, 6, 7, 8, 4, 5, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 13, 11, 12, 9, 10, 7, 8, 4, 5, 6, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 128, 128, 13, 12, 11, 0, 1, 2, 3, 6, 5, 4, 7, 8, 9, 10));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 14, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 25, 26, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 22, 21, 24, 23, 26, 25, 16, 128, 128, 128, 20, 128, 128, 128, 8, 7, 6, 5, 12, 11, 10, 9, 0, 1, 2, 3, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 13, 128, 19, 18, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 25, 26, 22, 21, 24, 23, 18, 17, 20, 19, 128, 13, 128, 15, 10, 9, 12, 11, 6, 5, 8, 7, 2, 3, 4, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 13, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 128, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 18, 17, 16, 19, 22, 21, 20, 23, 26, 25, 24, 5, 6, 7, 4, 3, 2, 1, 0, 13, 14, 15, 12, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 24, 25, 26, 23, 18, 17, 16, 19, 22, 21, 20, 9, 10, 11, 8, 13, 14, 15, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 24, 25, 21, 20, 23, 22, 17, 16, 19, 18, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][28]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 26, 27, 24, 25, 22, 23, 21, 19, 20, 17, 18, 128, 128, 14, 12, 13, 10, 11, 8, 9, 7, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 24, 25, 26, 27, 23, 21, 22, 17, 18, 19, 20, 128, 15, 128, 10, 11, 12, 13, 7, 8, 9, 3, 4, 5, 6, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 26, 27, 24, 25, 22, 23, 21, 19, 20, 17, 18, 16, 14, 15, 12, 13, 10, 11, 9, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 21, 22, 23, 24, 25, 26, 27, 20, 128, 128, 16, 17, 128, 128, 13, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 128, 128, 18, 19, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 25, 26, 27, 22, 21, 24, 23, 18, 17, 20, 19, 128, 15, 128, 11, 10, 13, 12, 7, 8, 9, 4, 5, 6, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 14, 12, 13, 10, 11, 8, 9, 7, 6, 4, 5, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 128, 14, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 27, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 23, 22, 25, 24, 27, 26, 17, 16, 128, 128, 21, 20, 128, 128, 9, 8, 7, 6, 13, 12, 11, 10, 1, 0, 3, 2, 5, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 128, 128, 19, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 26, 27, 23, 22, 25, 24, 19, 18, 21, 20, 128, 128, 128, 128, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 1, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 19, 18, 17, 16, 23, 22, 21, 20, 27, 26, 25, 24, 6, 7, 5, 4, 3, 2, 1, 0, 14, 15, 13, 12, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 25, 24, 27, 26, 19, 18, 17, 16, 23, 22, 21, 20, 10, 11, 9, 8, 14, 15, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 26, 27, 24, 25, 21, 20, 23, 22, 17, 16, 19, 18, 12, 13, 14, 15, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][29]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 14, 12, 13, 10, 11, 8, 9, 7, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 25, 26, 27, 28, 21, 22, 23, 24, 17, 18, 19, 20, 128, 15, 128, 10, 11, 12, 13, 7, 8, 9, 3, 4, 5, 6, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 16, 14, 15, 12, 13, 10, 11, 9, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 21, 22, 23, 24, 25, 26, 27, 28, 20, 128, 128, 16, 17, 128, 128, 13, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 15, 128, 128, 18, 19, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 26, 25, 28, 27, 22, 21, 24, 23, 18, 17, 20, 19, 128, 15, 128, 11, 10, 13, 12, 7, 8, 9, 4, 5, 6, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 14, 128, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 14, 12, 13, 10, 11, 8, 9, 7, 6, 4, 5, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 128, 128, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 14, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 27, 28, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 24, 23, 22, 25, 28, 27, 26, 17, 16, 128, 128, 21, 20, 128, 128, 9, 8, 7, 6, 13, 12, 11, 10, 1, 0, 3, 2, 5, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 128, 128, 19, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 26, 27, 28, 23, 22, 25, 24, 19, 18, 21, 20, 128, 128, 128, 128, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 17, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 128, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 128, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 1, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 28, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 14, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 128, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 20, 19, 18, 17, 16, 23, 22, 21, 28, 27, 26, 25, 24, 6, 7, 5, 4, 3, 2, 1, 0, 14, 15, 13, 12, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 24, 25, 26, 27, 28, 19, 18, 17, 16, 23, 22, 21, 20, 10, 11, 9, 8, 14, 15, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 24, 25, 26, 21, 20, 23, 22, 17, 16, 19, 18, 12, 13, 14, 15, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][30]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 15, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 26, 27, 28, 29, 22, 23, 24, 25, 18, 19, 20, 21, 128, 16, 128, 11, 12, 13, 14, 7, 8, 9, 10, 3, 4, 5, 6, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 17, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 22, 23, 24, 25, 26, 27, 28, 29, 21, 128, 16, 17, 18, 19, 128, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 128, 20, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 27, 26, 29, 28, 23, 22, 25, 24, 19, 18, 21, 20, 128, 16, 128, 12, 11, 14, 13, 8, 7, 10, 9, 4, 5, 6, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 15, 13, 14, 11, 12, 9, 10, 7, 8, 6, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 128, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 29, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 25, 24, 23, 26, 29, 28, 27, 18, 17, 16, 128, 22, 21, 20, 128, 10, 9, 8, 7, 14, 13, 12, 11, 2, 1, 0, 3, 6, 5, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 19, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 27, 28, 29, 24, 23, 26, 25, 20, 19, 22, 21, 16, 128, 18, 128, 12, 11, 14, 13, 8, 7, 10, 9, 4, 3, 6, 5, 0, 1, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 21, 20, 19, 18, 17, 16, 23, 22, 29, 28, 27, 26, 25, 24, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0x4e), 0xffffffff)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 28, 29, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
[1][31]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi8(uint8_t(0xff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 29, 30, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 27, 28, 29, 30, 23, 24, 25, 26, 19, 20, 21, 22, 128, 16, 17, 128, 11, 12, 13, 14, 7, 8, 9, 10, 3, 4, 5, 6, 2, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 18, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 29, 30, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 23, 24, 25, 26, 27, 28, 29, 30, 128, 16, 17, 18, 19, 20, 21, 128, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 128, 128, 128, 22, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 28, 27, 30, 29, 24, 23, 26, 25, 20, 19, 22, 21, 16, 128, 18, 128, 12, 11, 14, 13, 8, 7, 10, 9, 4, 5, 6, 1, 0, 3, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 29, 30, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 6, 4, 5, 2, 3, 0, 1));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 128, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 30, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 26, 25, 24, 23, 30, 29, 28, 27, 18, 17, 16, 128, 22, 21, 20, 128, 10, 9, 8, 7, 14, 13, 12, 11, 2, 1, 0, 3, 6, 5, 4));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 19, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 28, 27, 30, 29, 24, 23, 26, 25, 20, 19, 22, 21, 16, 128, 18, 128, 12, 11, 14, 13, 8, 7, 10, 9, 4, 3, 6, 5, 0, 1, 2));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 29, 30, 27, 28, 25, 26, 23, 24, 21, 22, 19, 20, 17, 18, 128, 128, 13, 14, 11, 12, 9, 10, 7, 8, 5, 6, 3, 4, 1, 2, 0));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 16, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(31, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 15, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(128, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 128, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 22, 21, 20, 19, 18, 17, 16, 23, 30, 29, 28, 27, 26, 25, 24, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 26, 25, 24, 27, 30, 29, 28, 19, 18, 17, 16, 23, 22, 21, 20, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x80, 0x80, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 28, 29, 30, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x0, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x0, 0x0))
[1][32]
DO_FULL_LOAD: True
_mm256_loadu_si256((__m256i *)[ARR])
_mm256_shuffle_epi8([V], _mm256_set_epi8(30, 31, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(28, 29, 30, 31, 24, 25, 26, 27, 20, 21, 22, 23, 16, 17, 18, 19, 12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(30, 31, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(24, 25, 26, 27, 28, 29, 30, 31, 16, 17, 18, 19, 20, 21, 22, 23, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x55)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(30, 31, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x33)
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x55)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(30, 31, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
__m256i [TMP0] = _mm256_permute4x64_epi64(v, 0x4e);
__m256i [TMP1] = _mm256_shuffle_epi8([V], _mm256_set_epi8(128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128));
__m256i [TMP2] = _mm256_shuffle_epi8([TMP0], _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31));
_mm256_or_si256([TMP1], [TMP2])
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0xf)
_mm256_shuffle_epi32([V], 0x4e)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x33)
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x55)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0, 0x80, 0x80, 0x0, 0x0))
_mm256_shuffle_epi8([V], _mm256_set_epi8(30, 31, 28, 29, 26, 27, 24, 25, 22, 23, 20, 21, 18, 19, 16, 17, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1))
_mm256_min_epu8([V1], [V2])
_mm256_min_epu8([V1], [V2])
_mm256_blendv_epi8([V1], [V2], _mm256_set_epi8(0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0, 0x80, 0x0))
[2][4]
DO_FULL_LOAD: True
__m64 [TMP0] = _mm_set1_pi16(uint16_t(0xffff));
memcpy(&[TMP0], [ARR], 8);
[TMP0]
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0x0)
__m64 [TMP0] = _mm_set1_pi16(1 << 15);
__m64 [TMP1] = _mm_cmpgt_pi16(_mm_xor_si64([V1], [TMP0]), _mm_xor_si64([V2], [TMP0]));
_mm_or_si64(_mm_and_si64([TMP1], [V2]), _mm_andnot_si64([TMP1], [V1]))
__m64 [TMP0] = _mm_set1_pi16(1 << 15);
__m64 [TMP1] = _mm_cmpgt_pi16(_mm_xor_si64([V1], [TMP0]), _mm_xor_si64([V2], [TMP0]));
_mm_or_si64(_mm_and_si64([TMP1], [V2]), _mm_andnot_si64([TMP1], [V1]))
__m64 [TMP0] = (__m64)0xffff0000ffff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0x1b), 0x0)
__m64 [TMP0] = _mm_set1_pi16(1 << 15);
__m64 [TMP1] = _mm_cmpgt_pi16(_mm_xor_si64([V1], [TMP0]), _mm_xor_si64([V2], [TMP0]));
_mm_or_si64(_mm_and_si64([TMP1], [V2]), _mm_andnot_si64([TMP1], [V1]))
__m64 [TMP0] = _mm_set1_pi16(1 << 15);
__m64 [TMP1] = _mm_cmpgt_pi16(_mm_xor_si64([V1], [TMP0]), _mm_xor_si64([V2], [TMP0]));
_mm_or_si64(_mm_and_si64([TMP1], [V2]), _mm_andnot_si64([TMP1], [V1]))
__m64 [TMP0] = (__m64)0xffffffff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0x0)
__m64 [TMP0] = _mm_set1_pi16(1 << 15);
__m64 [TMP1] = _mm_cmpgt_pi16(_mm_xor_si64([V1], [TMP0]), _mm_xor_si64([V2], [TMP0]));
_mm_or_si64(_mm_and_si64([TMP1], [V2]), _mm_andnot_si64([TMP1], [V1]))
__m64 [TMP0] = _mm_set1_pi16(1 << 15);
__m64 [TMP1] = _mm_cmpgt_pi16(_mm_xor_si64([V1], [TMP0]), _mm_xor_si64([V2], [TMP0]));
_mm_or_si64(_mm_and_si64([TMP1], [V2]), _mm_andnot_si64([TMP1], [V1]))
__m64 [TMP0] = (__m64)0xffff0000ffff;
_mm_or_si64(_mm_and_si64([TMP0], [V2]), _mm_andnot_si64([TMP0], [V1]))
[2][5]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 7, 6, 9, 8, 5, 4, 1, 0, 3, 2))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x9)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 5, 4, 7, 6, 9, 8, 3, 2, 1, 0))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x4)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 3, 2, 5, 4, 7, 6, 9, 8, 1, 0))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x6)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0x1b), 0xe4)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x1)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xe4)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x5)
[2][6]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xd8), 0xe1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x12)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 7, 6, 9, 8, 11, 10, 5, 4, 1, 0, 3, 2))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x9)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 11, 10, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0xa)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 13, 12, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 1, 0))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x6)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0x4e), 0xe1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x13)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xe4)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x5)
[2][7]
DO_FULL_LOAD: False
__m128i [TMP0] = _mm_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0);
__m128i [TMP1] = _mm_maskload_epi32((int32_t * const)[ARR], _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm_or_si128([TMP1], [TMP0])
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x2a)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 7, 6, 9, 8, 11, 10, 13, 12, 5, 4, 1, 0, 3, 2))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x19)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x2a)
_mm_shuffle_epi8([V], _mm_set_epi8(15, 14, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x7)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0x4e), 0xc6)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x13)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xe1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x15)
[2][8]
DO_FULL_LOAD: True
_mm_loadu_si128((__m128i *)[ARR])
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xb1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x55)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0x1b), 0x1b)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x3)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xb1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x55)
_mm_shuffle_epi8([V], _mm_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14))
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x1)
_mm_shuffle_epi32([V], 0xb1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x3)
_mm_shufflehi_epi16(_mm_shufflelo_epi16([V], 0xb1), 0xb1)
_mm_min_epu16([V1], [V2])
_mm_min_epu16([V1], [V2])
_mm_blend_epi16([V1], [V2], 0x55)
[2][9]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 15, 14, 17, 16, 13, 12, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x95)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 13, 12, 15, 14, 17, 16, 11, 10, 9, 8, 1, 0, 3, 2, 5, 4, 7, 6))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x43)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 11, 10, 13, 12, 15, 14, 17, 16, 9, 8, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x65)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 5, 4, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x38)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 7, 6, 5, 4, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x50)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x1)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x55)
[2][10]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 16, 19, 18, 15, 14, 11, 10, 13, 12, 7, 6, 9, 8, 5, 4, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x129)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 15, 14, 17, 16, 19, 18, 13, 12, 11, 10, 9, 8, 5, 4, 7, 6, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x84)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 13, 12, 15, 14, 17, 16, 19, 18, 11, 10, 7, 6, 9, 8, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xc9)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 11, 10, 13, 12, 15, 14, 17, 16, 5, 4, 3, 2, 9, 8, 7, 6, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x66)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 15, 14, 17, 16, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xaa)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 7, 6, 9, 8, 3, 2, 5, 4, 11, 10, 17, 16, 19, 18, 13, 12, 15, 14, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1e)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 16, 19, 18, 15, 14, 13, 12, 7, 6, 1, 0, 11, 10, 5, 4, 3, 2, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x109)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x55)
[2][11]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 19, 18, 21, 20, 17, 16, 13, 12, 15, 14, 11, 10, 7, 6, 9, 8, 5, 4, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x249)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 17, 16, 19, 18, 21, 20, 15, 14, 11, 10, 13, 12, 9, 8, 5, 4, 7, 6, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x124)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 16, 19, 18, 13, 12, 15, 14, 11, 10, 7, 6, 9, 8, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x149)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 13, 12, 15, 14, 17, 16, 19, 18, 21, 20, 11, 10, 5, 4, 3, 2, 9, 8, 7, 6, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xc6)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 19, 18, 21, 20, 13, 12, 11, 10, 17, 16, 15, 14, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x26a)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 5, 4, 7, 6, 15, 14, 17, 16, 11, 10, 13, 12, 9, 8, 19, 18, 21, 20, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xac)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 9, 8, 3, 2, 5, 4, 7, 6, 17, 16, 11, 10, 13, 12, 15, 14, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1e)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 17, 16, 19, 18, 21, 20, 11, 10, 13, 12, 15, 14, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x123)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 16, 19, 18, 15, 14, 9, 8, 11, 10, 13, 12, 7, 6, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x111)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x55)
[2][12]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 20, 23, 22, 19, 18, 15, 14, 17, 16, 13, 12, 9, 8, 11, 10, 7, 6, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x492)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 18, 21, 20, 23, 22, 17, 16, 13, 12, 15, 14, 11, 10, 7, 6, 9, 8, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x249)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 19, 18, 21, 20, 15, 14, 17, 16, 13, 12, 9, 8, 11, 10, 7, 6, 5, 4, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x291)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 13, 12, 11, 10, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x183)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 20, 23, 22, 15, 14, 13, 12, 19, 18, 17, 16, 7, 6, 5, 4, 11, 10, 9, 8, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x4cd)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 17, 16, 19, 18, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x154)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 5, 4, 7, 6, 9, 8, 11, 10, 15, 14, 13, 12, 17, 16, 19, 18, 21, 20, 23, 22, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 18, 17, 16, 23, 22, 21, 20, 5, 4, 7, 6, 3, 2, 1, 0, 13, 12, 15, 14, 11, 10, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 21, 20, 23, 22, 17, 16, 19, 18, 9, 8, 11, 10, 13, 12, 15, 14, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x533)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x55)
[2][13]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 23, 22, 25, 24, 19, 18, 21, 20, 15, 14, 17, 16, 13, 12, 9, 8, 11, 10, 7, 6, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xa92)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 19, 18, 21, 20, 23, 22, 25, 24, 17, 16, 13, 12, 15, 14, 11, 10, 7, 6, 9, 8, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x649)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 23, 22, 25, 24, 19, 18, 21, 20, 15, 14, 17, 16, 13, 12, 9, 8, 11, 10, 7, 6, 5, 4, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xa91)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 13, 12, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 11, 10, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1c3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 21, 20, 23, 22, 25, 24, 15, 14, 13, 12, 19, 18, 17, 16, 7, 6, 5, 4, 11, 10, 9, 8, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x4cd)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 3, 2, 21, 20, 23, 22, 17, 16, 19, 18, 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 25, 24, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x556)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 5, 4, 7, 6, 9, 8, 11, 10, 15, 14, 13, 12, 17, 16, 19, 18, 21, 20, 23, 22, 3, 2, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 5, 4, 7, 6, 3, 2, 1, 0, 13, 12, 15, 14, 11, 10, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 17, 16, 19, 18, 21, 20, 9, 8, 11, 10, 13, 12, 15, 14, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x133)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xffffffff)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x555)
[2][14]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, 15, 14, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x152a)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 21, 20, 23, 22, 25, 24, 27, 26, 19, 18, 15, 14, 17, 16, 7, 6, 9, 8, 11, 10, 13, 12, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xc99)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, 15, 14, 11, 10, 13, 12, 7, 6, 9, 8, 5, 4, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1529)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 13, 12, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x387)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 23, 22, 25, 24, 27, 26, 17, 16, 15, 14, 21, 20, 19, 18, 9, 8, 7, 6, 13, 12, 11, 10, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x999)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 23, 22, 25, 24, 19, 18, 21, 20, 15, 14, 17, 16, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0xaaa)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x7e)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 19, 18, 17, 16, 23, 22, 21, 20, 27, 26, 25, 24, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0x4e), 0xffffffff)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1333)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xffffffff)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x555)
[2][15]
DO_FULL_LOAD: False
__m256i [TMP0] = _mm256_set_epi16(uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), uint16_t(0xffff), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
__m256i [TMP1] = _mm256_maskload_epi32((int32_t * const)[ARR], _mm256_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000));
_mm256_or_si256([TMP1], [TMP0])
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 27, 26, 29, 28, 23, 22, 25, 24, 19, 18, 21, 20, 15, 14, 17, 16, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x2aaa)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 23, 22, 25, 24, 27, 26, 29, 28, 15, 14, 17, 16, 19, 18, 21, 20, 7, 6, 9, 8, 11, 10, 13, 12, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1999)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 27, 26, 29, 28, 23, 22, 25, 24, 19, 18, 21, 20, 15, 14, 17, 16, 11, 10, 13, 12, 7, 6, 9, 8, 5, 4, 1, 0, 3, 2))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x2aa9)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29, 28, 13, 12, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x787)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 25, 24, 23, 22, 29, 28, 27, 26, 17, 16, 15, 14, 21, 20, 19, 18, 9, 8, 7, 6, 13, 12, 11, 10, 1, 0, 3, 2, 5, 4))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1999)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 27, 26, 29, 28, 23, 22, 25, 24, 19, 18, 21, 20, 15, 14, 17, 16, 11, 10, 13, 12, 7, 6, 9, 8, 3, 2, 5, 4, 1, 0))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x2aaa)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29, 28))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x7f)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 21, 20, 19, 18, 17, 16, 23, 22, 29, 28, 27, 26, 25, 24, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x70f)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0x4e), 0xffffffff)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1333)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xffffffff)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x1555)
[2][16]
DO_FULL_LOAD: True
_mm256_loadu_si256((__m256i *)[ARR])
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x5555)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0x1b), 0x1b)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0xf)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x5555)
_mm256_shuffle_epi8([V], _mm256_set_epi8(17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29, 28, 31, 30, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0xf)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x5555)
_mm256_shuffle_epi8([V], _mm256_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29, 28, 31, 30))
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi32([V], 0x4e)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0xf)
_mm256_shufflehi_epi16(_mm256_shufflelo_epi16([V], 0xb1), 0xb1)
_mm256_min_epu16([V1], [V2])
_mm256_min_epu16([V1], [V2])
_mm256_blend_epi16([V1], [V2], 0x5555)
[4][4]
DO_FULL_LOAD: True
_mm_loadu_si128((__m128i *)[ARR])
_mm_shuffle_epi32([V], 0xb1)
_mm_min_epu32([V1], [V2])
_mm_min_epu32([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x5)
_mm_shuffle_epi32([V], 0x1b)
_mm_min_epu32([V1], [V2])
_mm_min_epu32([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x3)
_mm_shuffle_epi32([V], 0xb1)
_mm_min_epu32([V1], [V2])
_mm_min_epu32([V1], [V2])
_mm_blend_epi32([V1], [V2], 0x5)
[4][5]
DO_FULL_LOAD: False
_mm256_maskload_epi32(_mm256_set_epi_32(0x0, 0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 15, 14, 13, 12, 19, 18, 17, 16, 11, 10, 9, 8, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x9)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 11, 10, 9, 8, 15, 14, 13, 12, 19, 18, 17, 16, 7, 6, 5, 4, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x4)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12, 19, 18, 17, 16, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x6)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x3)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
[4][6]
DO_FULL_LOAD: False
_mm256_maskload_epi32(_mm256_set_epi_32(0x0, 0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 18, 17, 16, 23, 22, 21, 20, 15, 14, 13, 12, 7, 6, 5, 4, 11, 10, 9, 8, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x12)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 15, 14, 13, 12, 19, 18, 17, 16, 23, 22, 21, 20, 11, 10, 9, 8, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x9)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 15, 14, 13, 12, 19, 18, 17, 16, 7, 6, 5, 4, 11, 10, 9, 8, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0xa)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12, 19, 18, 17, 16, 23, 22, 21, 20, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x6)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 18, 17, 16, 23, 22, 21, 20, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x5)
[4][7]
DO_FULL_LOAD: False
_mm256_maskload_epi32(_mm256_set_epi_32(0x0, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000))
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 23, 22, 21, 20, 27, 26, 25, 24, 15, 14, 13, 12, 19, 18, 17, 16, 7, 6, 5, 4, 11, 10, 9, 8, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x2a)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 15, 14, 13, 12, 19, 18, 17, 16, 23, 22, 21, 20, 27, 26, 25, 24, 11, 10, 9, 8, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x19)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 23, 22, 21, 20, 27, 26, 25, 24, 15, 14, 13, 12, 19, 18, 17, 16, 7, 6, 5, 4, 11, 10, 9, 8, 3, 2, 1, 0))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x2a)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12, 19, 18, 17, 16, 23, 22, 21, 20, 27, 26, 25, 24))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x7)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 19, 18, 17, 16, 23, 22, 21, 20, 27, 26, 25, 24, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x13)
_mm256_shuffle_epi8([V], _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 19, 18, 17, 16, 23, 22, 21, 20, 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x15)
[4][8]
DO_FULL_LOAD: True
_mm256_loadu_si256((__m256i *)[ARR])
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x55)
_mm256_shuffle_epi32([V], 0x1b)
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x33)
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x55)
_mm256_shuffle_epi8([V], _mm256_set_epi8(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12, 19, 18, 17, 16, 23, 22, 21, 20, 27, 26, 25, 24, 31, 30, 29, 28))
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0xf)
_mm256_shuffle_epi32([V], 0x4e)
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x33)
_mm256_shuffle_epi32([V], 0xb1)
_mm256_min_epu32([V1], [V2])
_mm256_min_epu32([V1], [V2])
_mm256_blend_epi32([V1], [V2], 0x55)
[8][4]
DO_FULL_LOAD: True
_mm256_loadu_si256((__m256i *)[ARR])
_mm256_shuffle_epi32([V], 0x4e)
__m256i [TMP0] = _mm256_set1_epi64x(1UL) << 63);
__m256i [TMP1] = _mm256_cmpgt_epi64(_mm256_xor_si256([V1], [TMP0]), _mm256_xor_si256([V2], [TMP0]));
_mm256_blendv_epi8([V1], [V2], [TMP1])
__m256i [TMP0] = _mm256_set1_epi64x(1UL) << 63);
__m256i [TMP1] = _mm256_cmpgt_epi64(_mm256_xor_si256([V1], [TMP0]), _mm256_xor_si256([V2], [TMP0]));
_mm256_blendv_epi8([V1], [V2], [TMP1])
_mm256_blend_epi64([V1], [V2], 0x33)
_mm256_permute4x64_epi64([V], 0xe4)
__m256i [TMP0] = _mm256_set1_epi64x(1UL) << 63);
__m256i [TMP1] = _mm256_cmpgt_epi64(_mm256_xor_si256([V1], [TMP0]), _mm256_xor_si256([V2], [TMP0]));
_mm256_blendv_epi8([V1], [V2], [TMP1])
__m256i [TMP0] = _mm256_set1_epi64x(1UL) << 63);
__m256i [TMP1] = _mm256_cmpgt_epi64(_mm256_xor_si256([V1], [TMP0]), _mm256_xor_si256([V2], [TMP0]));
_mm256_blendv_epi8([V1], [V2], [TMP1])
_mm256_blend_epi64([V1], [V2], 0xf)
_mm256_shuffle_epi32([V], 0x4e)
__m256i [TMP0] = _mm256_set1_epi64x(1UL) << 63);
__m256i [TMP1] = _mm256_cmpgt_epi64(_mm256_xor_si256([V1], [TMP0]), _mm256_xor_si256([V2], [TMP0]));
_mm256_blendv_epi8([V1], [V2], [TMP1])
__m256i [TMP0] = _mm256_set1_epi64x(1UL) << 63);
__m256i [TMP1] = _mm256_cmpgt_epi64(_mm256_xor_si256([V1], [TMP0]), _mm256_xor_si256([V2], [TMP0]));
_mm256_blendv_epi8([V1], [V2], [TMP1])
_mm256_blend_epi64([V1], [V2], 0x33)
